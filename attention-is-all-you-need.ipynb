{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b360ae-96db-469a-a300-c8cf6975524b",
   "metadata": {},
   "source": [
    "# Attention Is All You Need Boiled Down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474f951-839a-4abb-a699-00d708b56bca",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Reccurent nueral architectures in NLP are awesome and everything, but they don't enable paralell computation and that's not so good from a computational efficiency perspective. That's why they are proposing the Transformer architecture, that models the dependency same as or even better than RNNs, and enable parallel computation.\n",
    "\n",
    "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_02_13B-RecurrentNeuralNetworks-WHITEBG.png\" style=\"width: 70%; position: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201a2a9-0a1d-4967-9836-4898d68fa389",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Some other architectures tried to minimize the sequential computation of RNN by using convolutions, then feeding their output to RNNs. But these architectures fail to model dependencies in parts of the sentences which are away from each other.\n",
    "\n",
    "Other architectures also utilized self attention, but the Transform architecture is the first to used full self-attention to model relationships and produce outputs based only on the self attention without sequential modelling.\n",
    "\n",
    "![self attention RNN](https://lilianweng.github.io/posts/2018-06-24-attention/encoder-decoder-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047a181-5ef1-4b6e-a137-a600bebb469e",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Language models use encoder-decoder architecture, where encoder uses `x` input to produce continous `z` output, and the decoder uses `z` to produce `y = (y1, y2, y3, ...)` sequentially one step at a time, each time consuming the previously generated outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1adea-cc28-422a-82f5-300e61eda886",
   "metadata": {},
   "source": [
    "## Encoder - Decoder \n",
    "\n",
    "![arch](https://i.stack.imgur.com/eAKQu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5de77-10c7-4680-8659-dfbc25253adc",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The attention used in the encoder-deocder block is Multi-Head Atention, consisting of multiple Scale Dot Product attention.\n",
    "\n",
    "![mha](https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c2f98b-9524-40ea-b878-ca9da60fedf7",
   "metadata": {},
   "source": [
    "## Simple Attention Layer \n",
    "\n",
    "In a encoder block, self attention is calculated from the output of the previous layer. Which means that `Q, K, V = X`.\n",
    "\n",
    "Let's imagine having a batch of 1 sequence of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5b4342-7e05-4ac3-8174-bef8cc998892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63475b1-686e-431c-94a9-5588f2e0a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((16, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dde65d7-7779-4655-8272-8144ae13001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, K, V = X, X, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "680c5f7a-9ff3-4b9a-83af-acd8888bd41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 32]), torch.Size([16, 32]), torch.Size([16, 32]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45801180-6c58-4f91-8890-58e69c18148e",
   "metadata": {},
   "source": [
    "![sda](https://www.tutorialexample.com/wp-content/uploads/2020/10/Scaled-Dot-Product-Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bfc7357-dfe5-4301-bb8a-467e736f264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = torch.matmul(Q, K.T)\n",
    "denom = Q.shape[1]**-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b744924a-0190-45b8-bc0d-2b3f95f37479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V):\n",
    "    matmul = torch.matmul(Q, K.T)\n",
    "    scale = num/Q.shape[1]**-2\n",
    "    softmax = torch.nn.Softmax(dim=1)(scale)\n",
    "    return torch.matmul(softmax, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e0e87bb-48e2-49f6-8158-5040c156bbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(Q, K, V).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634649a1-ad69-4f8b-8d78-bffe7721e003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
