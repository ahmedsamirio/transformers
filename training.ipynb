{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa082eb-24a0-47d7-8af6-b03a07ff049d",
   "metadata": {},
   "source": [
    "This notebook will contain the final version of all code written to train the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5b23c0-ef74-4201-ad20-944bff16064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a8c405-1640-4b55-ad42-97ae407da9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, de, in_d, debug=True):\n",
    "        super().__init__()\n",
    "        self.W1 = torch.nn.Linear(de, in_d)\n",
    "        self.W2 = torch.nn.Linear(in_d, de)\n",
    "        self.layernorm = torch.nn.LayerNorm(de)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.debug = debug\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.debug:\n",
    "            print('MLP')\n",
    "            print('Before W')\n",
    "            print('x', x.shape)\n",
    "            \n",
    "        x = x + self.W2(torch.nn.functional.relu(self.W1(x)))\n",
    "        \n",
    "        if self.debug:\n",
    "            print('After W')\n",
    "            print('x', x.shape)\n",
    "            \n",
    "        x = self.layernorm(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980b6217-eedf-4701-bb14-d87d37505e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "    def __init__(self, h, de, dq, dk, dv, debug=True):\n",
    "        super().__init__()\n",
    "        self.h, self.de, self.dq, self.dk, self.dv, self.debug = h, de, dq, dk, dv, debug\n",
    "        self.W_q = torch.nn.Linear(de, dq*h)\n",
    "        self.W_k = torch.nn.Linear(de, dk*h)\n",
    "        self.W_v = torch.nn.Linear(de, dv*h)\n",
    "        self.W_o = torch.nn.Linear(h*dv, de) \n",
    "        self.layernorm = torch.nn.LayerNorm(de)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "    def scaled_dot_prod_attention(self, q, k, v, masks=None):\n",
    "        qkt = torch.einsum('bhqd, bhkd -> bhqk', q, k)\n",
    "        scale = qkt/(q.shape[-1]**-2)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('qkt/scale', scale.shape)\n",
    "        \n",
    "        if masks is not None:\n",
    "            if self.debug:\n",
    "                print('masks', masks[:, None, None, :].shape)\n",
    "            \n",
    "            scale = scale.masked_fill(masks[:, None, None, :], -torch.inf)\n",
    "        \n",
    "        soft = torch.nn.Softmax(dim=-1)(scale)\n",
    "        return torch.einsum('bhij, bhjk -> bhik', soft, v)\n",
    "        \n",
    "    def forward(self, x, encoder_output=None, masks=None):\n",
    "        if self.debug:\n",
    "            print('x', x.shape)\n",
    "        \n",
    "        if encoder_output is not None:\n",
    "            # I don't know if the encoder_output should be linearly transformed\n",
    "            # So I need to test this later on \n",
    "            q, k, v = self.W_q(x), self.W_k(encoder_output), self.W_v(encoder_output)\n",
    "        else:\n",
    "            q, k, v = self.W_q(x), self.W_k(x), self.W_v(x)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Before splitting')\n",
    "            print('q', q.shape)\n",
    "            print('k', k.shape)\n",
    "            print('v', v.shape)\n",
    "        \n",
    "        # Split into different heads\n",
    "        bs = q.shape[0]\n",
    "        ln_q = q.shape[1]\n",
    "        ln_k = k.shape[1]\n",
    "        ln_v = v.shape[1]\n",
    "\n",
    "        q = q.view(bs, ln_q, self.h, self.dq)\n",
    "        k = k.view(bs, ln_k, self.h, self.dk)\n",
    "        v = v.view(bs, ln_v, self.h, self.dv)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('After splitting')\n",
    "            print('q', q.shape)\n",
    "            print('k', k.shape)\n",
    "            print('v', v.shape)\n",
    "        \n",
    "        # Transpose ln and h for scaled dot product attention\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('After Transpose')\n",
    "            print('q', q.shape)\n",
    "            print('k', k.shape)\n",
    "            print('v', v.shape)\n",
    "        \n",
    "        sdpa = self.scaled_dot_prod_attention(q, k, v, masks)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('After scaled dot product')\n",
    "            print('sdpa', sdpa.shape)\n",
    "        \n",
    "        # Linear projection of attention \n",
    "        sdpa = self.W_o(sdpa.view(bs, ln_q, -1))\n",
    "        \n",
    "        if self.debug:\n",
    "            print('After linear projection')\n",
    "            print('sdpa', sdpa.shape)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = (sdpa + x).view(bs, ln_q, -1)\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('After add & norm')\n",
    "            print('x', x.shape)\n",
    "        \n",
    "        x = self.dropout(x.contiguous().view(bs, ln_q, self.de))\n",
    "        \n",
    "        if self.debug:\n",
    "            print('After concat')\n",
    "            print('x', x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e39f17-7d0e-4d5d-98f7-d91ce5baa84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, in_d, h, de, dq, dk, dv, debug=True):\n",
    "        super().__init__()\n",
    "        self.mla = MultiHeadedAttention(h, de, dq, dk, dv, debug)\n",
    "        self.mlp = MLP(de, in_d, debug)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(self.mla(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4d4ac2-5fc4-44c2-a3c4-61796ea2b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, in_d, h, de, dq, dk, dv, debug=True):\n",
    "        super().__init__()\n",
    "        self.mla1 = MultiHeadedAttention(h, de, dq, dk, dv, debug)\n",
    "        self.mla2 = MultiHeadedAttention(h, de, dq, dk, dv, debug)\n",
    "        self.mlp = MLP(de, in_d, debug)\n",
    "        \n",
    "    def forward(self, x, encoder_output, masks):\n",
    "        x = self.mla1(x, masks=masks)\n",
    "        x = self.mla2(x, encoder_output=encoder_output)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b2defc-6793-4958-a329-624e1269ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, n_vocab_inp, dmodel=512, dmiddle=2048, dq=64, dk=64, dv=64, h=8, n_encoders=6, n_decoders=6, debug=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dmodel = dmodel\n",
    "        self.input_embeddings = torch.nn.Embedding(num_embeddings=n_vocab_inp, embedding_dim=dmodel)\n",
    "        self.encoder_blocks = torch.nn.ModuleList([EncoderBlock(dmiddle, h, dmodel, dq, dk, dv, debug) for _ in range(n_encoders)])\n",
    "        self.decoder_blocks = torch.nn.ModuleList([DecoderBlock(dmiddle, h, dmodel, dq, dk, dv, debug) for _ in range(n_decoders)])\n",
    "        \n",
    "    def forward(self, x, y, masks):\n",
    "        x = self.input_embeddings(x)\n",
    "        x = self.add_positional_encodings(x)\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder(x)\n",
    "        \n",
    "        \n",
    "        y = self.input_embeddings(y)\n",
    "        y = self.add_positional_encodings(y)\n",
    "        for decoder in self.decoder_blocks:\n",
    "            y = decoder(y, x, masks)\n",
    "            \n",
    "        final = self.final_linear(y)\n",
    "        \n",
    "        return final\n",
    "    \n",
    "    def add_positional_encodings(self, x):\n",
    "        pos, i = torch.where(x[0])\n",
    "        sin_enc = torch.sin(pos/(10000**(2*i/self.dmodel)))\n",
    "        cos_enc = torch.cos(pos/(10000**(2*i/self.dmodel)))\n",
    "        pos_enc = torch.where(i % 2 == 0, sin_enc, cos_enc).view(x.shape[1:])\n",
    "        return x + pos_enc\n",
    "    \n",
    "    def final_linear(self, y):\n",
    "        return torch.einsum('bmd, dn -> bmn', y, self.input_embeddings.weight.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8407961-478a-4645-a34b-1f2c2274355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):            \n",
    "        tokenized_input = tokenizer.encode(self.data[idx]['translation']['en'])\n",
    "        tokenized_output = tokenizer.encode(self.data[idx]['translation']['ar'])\n",
    "\n",
    "        encoder_input_ids = torch.tensor([tokenized_input.ids for _ in range(len(tokenized_output.ids)-1)])[:, 1:-1]\n",
    "        \n",
    "        tokenized_output_ids = torch.tensor([tokenized_output.ids for _ in range(len(tokenized_output.ids)-1)])\n",
    "        \n",
    "        decoder_input_ids = tokenized_output_ids[:, :-1]\n",
    "        decoder_input_masks = self.get_decoder_masks(decoder_input_ids)\n",
    "        \n",
    "        \n",
    "        target_ids = tokenized_output_ids[:, 1:]\n",
    "        target_masks = self.get_target_masks(target_ids)\n",
    "        \n",
    "        return encoder_input_ids, decoder_input_ids, target_ids, decoder_input_masks, target_masks\n",
    "        \n",
    "\n",
    "    def get_decoder_masks(self, ids):\n",
    "        masks = torch.ones_like(ids)\n",
    "        return torch.triu(masks, diagonal=1)\n",
    "    \n",
    "    def get_target_masks(self, ids):\n",
    "        masks = torch.ones_like(ids)\n",
    "        return torch.tril(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a4b291-ed3e-4220-8867-3b3c30d1affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a7296c-093f-4b62-a836-7428cd17f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset news_commentary (data/news_commentary/ar-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345f8b0370af409dbc12e722580334db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"news_commentary\", \"ar-en\", split=[\"train\"], cache_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2f4ccf4-d150-40ac-b5bc-8afd24d4ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080b5279-a379-4eed-bf55-3cecf2153d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.NFD(), \n",
    "        normalizers.Lowercase(), \n",
    "        normalizers.StripAccents()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eafcd237-ab24-4830-862b-b06197e0b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87bb8d6-9693-47e3-9811-2b12beef9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1afac65f-2f5b-4f88-823c-c120d01d0a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['الذهب بعشرة آلاف دولار؟',\n",
       " '$10,000 Gold?',\n",
       " 'سان فرانسيسكو ـ لم يكن من السهل قط أن ينخرط المرء في محادثة عقلانية حول قيمة الذهب. ومؤخراً، مع ارتفاع أسعار الذهب بما يزيد على 300% في غضون الأعوام العشرة الماضية، فقد أصبح الأمر أصعب من أي وقت مضى. ففي شهر ديسمبر/كانون الأول الماضي، نشر كل من مارتن فيلدشتاين ونورييل روبيني ـ وهما من أبرز خبراء الاقتصاد ـ مقالاً تحريرياً، حيث شكك كل منهما في مقاله بشجاعة في مشاعر السوق الصاعدة، مشيراً بحكمة إلى المجازفات والمخاطر المحيطة بالذهب.',\n",
       " 'Lately, with gold prices up more than 300% over the last decade, it is harder than ever. Just last December, fellow economists Martin Feldstein and Nouriel Roubini each penned op-eds bravely questioning bullish market sentiment, sensibly pointing out gold’s risks.',\n",
       " 'ولكن من المؤسف أن أسعار الذهب واصلت ارتفاعها منذ نشر المقالين. حتى أن أسعار الذهب سجلت رقماً قياسياً بلغ 1300 دولار مؤخرا. وفي ديسمبر/كانون الأول الماضي كان العديد من أنصار الذهب يزعمون أن الأسعار تتجه حتماً نحو 2000 دولار. والآن، وبتشجيع من الارتفاع المستمر في قيمة الذهب، يزعم البعض أن أسعار الذهب تتجه إلى الارتفاع إلى ما هو أعلى من ذلك الرقم.',\n",
       " 'Last December, many gold bugs were arguing that the price was inevitably headed for $2,000. Now, emboldened by continuing appreciation, some are suggesting that gold could be headed even higher than that.',\n",
       " 'ومؤخراً شرح لي أحد المستثمرين الناجحين في مجال الذهب أن أسعار الأسهم كانت هزيلة لأكثر من عشرة أعوام قبل أن يتجاوز مؤشر داو جونز علامة الألف في أوائل ثمانينيات القرن العشرين. ومنذ ذلك الوقت استمر المؤشر في الصعود حتى تجاوز العشرة آلاف نقطة. والآن بعد أن عبر الذهب حاجز الألف السحري، فما الذي يمنعه من الارتفاع إلى عشرة أمثال؟',\n",
       " 'One successful gold investor recently explained to me that stock prices languished for a more than a decade before the Dow Jones index crossed the 1,000 mark in the early 1980’s. Since then, the index has climbed above 10,000.',\n",
       " 'لا أحد يستطيع أن ينكر أن توقع ارتفاع أسعار الذهب إلى مستويات أعلى كثيراً من مستوياته الحالية ليس بالخيال المحض كما قد يبدو الأمر. فبعد تعديل الأسعار الحالية وفقاً للتضخم، سوف يتبين لنا أنها لم تقترب حتى من الارتفاع القياسي الذي سجلته في يناير/كانون الثاني 1980. فآنذاك بلغ سعر الذهب 850 دولار، أي أكثر من 2000 دولار بالقيمة الحالية للدولار. ولكن ما حدث في يناير/كانون الثاني 1980 كان \"ارتفاعاً شاذا\"أثناء فترة اتسمت بانعدام الاستقرار السياسي الجغرافي. وعند مستوى 1300 دولار، فإن أسعار اليوم ربما كانت أكثر من ضعف متوسط السعر المعدل وفقاً للتضخم في الأمد البعيد. ما الذي قد يبرر إذن أي زيادة كبيرة أخرى في أسعار الذهب نسبة إلى مستوياتها الحالية؟',\n",
       " 'But January 1980 was arguably a “freak peak” during a period of heightened geo-political instability. At $1,300, today’s price is probably more than double very long-term, inflation-adjusted, average gold prices.',\n",
       " 'من بين الإجابات على هذا التساؤل بالطبع، الانهيار الكامل للدولار الأميركي. ففي ظل مستويات العجز التي ارتفعت إلى عنان السماء، والسياسة المالية التي فقدت الدفة التي توجهها، لا يملك المرء إلا أن يتساءل ما إذا كانت إحدى الإدارات الشعوبية قد تلجأ بتهور إلى طباعة أوراق النقد. وإن كنتم تشعرون بالقلق إزاء هذا الاحتمال فإن الذهب قد يكون حقاً وسيلة التحوط الأكثر جدارة بالثقة.',\n",
       " 'With soaring deficits, and a rudderless fiscal policy, one does wonder whether a populist administration might recklessly turn to the printing press. And if you are really worried about that, gold might indeed be the most reliable hedge.',\n",
       " 'لا شك أن البعض قد يزعمون أن السندات المعدلة وفقاً للتضخم تشكل وسيلة أفضل وأكثر مباشرة في التحوط من التضخم مقارنة بالذهب. ولكن أنصار الذهب محقون حين يعربون عن قلقهم بشأن ما إذا كانت الحكومة سوف تحترم تعهداتها في ظل ظروف أكثر شِدة. وكما أشرت أنا وكارمن راينهارت في كتابنا الذي نشر مؤخراً عن تاريخ الأزمات المالية تحت عنوان \"هذه المرة مختلفة\"، فإن الحكومات التي تعاني من عجز في النقود السائلة قد تلجأ قسراً في الكثير من الأحيان إلى تحويل الديون المعدلة إلى ديون غير معدلة وفقاً للتضخم، حتى يصبح في الإمكان خفض قيمة هذه الديون بفعل التضخم. وحتى الولايات المتحدة ألغت الفقرات الخاصة بتعديل القيمة وفقاً للتضخم في عقود السندات أثناء أزمة الكساد الأعظم في ثلاثينيات القرن العشرين. أي أن هذا قد يتكرر في أي مكان.',\n",
       " 'But gold bugs are right to worry about whether the government will honor its commitments under more extreme circumstances. In fact, as Carmen Reinhart and I discuss in our recent book on the history of financial crises, This Time is Different, cash-strapped governments will often forcibly convert indexed debt to non-indexed debt, precisely so that its value might be inflated away.',\n",
       " 'ورغم ذلك، فمجرد إمكانية حدوث التضخم البالغ الارتفاع لا تجعله أمراً محتملاً، لذا يتعين على المرء أن يتوخى الحذر حين يزعم أن ارتفاع أسعار الذهب يرجع إلى توقعات التضخم. ولقد زعم البعض بدلاً من ذلك أن مسيرة الذهب الصاعدة الطولية كانت مدفوعة جزئياً بتطوير الأدوات المالية الجديدة التي تعمل على تيسير المتاجرة والمضاربة في الذهب.',\n",
       " 'Even so, the fact that very high inflation is possible does not make it probable, so one should be cautious in arguing that higher gold prices are being driven by inflation expectations. Some have argued instead that gold’s long upward march has been partly driven by the development of new financial instruments that make it easier to trade and speculate in gold.',\n",
       " 'وقد يكون في هذه الحجة مسحة من الصدق ـ وقد تنطوي أيضاً على قدر من المفارقة. فقد انهمك علماء الكيمياء في القرون الوسطى فيما نعتبره اليوم بحثاً عبثياً عن السبل الكفيلة بتحويل المعادن الخسيسة إلى ذهب. أليس من قبيل المفارقة إذن أن تتمكن الخيمياء المالية من الزيادة من قيمة سبيكة من الذهب إلى مستويات بالغة الارتفاع؟',\n",
       " 'After all, medieval alchemists engaged in what we now consider an absurd search for ways to transform base metals into gold. Wouldn’t it be paradoxical, then, if financial alchemy could make an ingot of gold worth dramatically more?',\n",
       " 'في اعتقادي أن الحجة الأكثر إقناعاً لتبرير المستويات المرتفعة التي بلغتها أسعار الذهب اليوم تتلخص في الصعود المثير الذي حققته أسواق آسيا وأميركا اللاتينية والشرق الأوسط في الاقتصاد العالمي. ومع اكتساب جحافل من المستهلكين الجدد للقوة الشرائية، فإن الطلب سوف يرتفع حتماً، الأمر الذي لابد وأن يدفع أسعار السلع الأساسية النادرة إلى الارتفاع.',\n",
       " 'In my view, the most powerful argument to justify today’s high price of gold is the dramatic emergence of Asia, Latin America, and the Middle East into the global economy. As legions of new consumers gain purchasing power, demand inevitably rises, driving up the price of scarce commodities.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "list(chain.from_iterable(map(lambda x: x.values(), dataset[0][0:10]['translation'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd1650dc-db21-4593-9c67-8892c3371b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "def batch_iterator():\n",
    "    for i in range(len(dataset)):\n",
    "        for j in range(0, len(dataset[i]), batch_size):\n",
    "            yield list(\n",
    "                chain.from_iterable(\n",
    "                    map(lambda x: x.values(),\n",
    "                        dataset[0][0:batch_size]['translation'])\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13397519-ad73-4f6d-b836-114fb1d79a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(batch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85fcb25f-6467-4a20-8cfd-472a2663476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b889c9b-e9aa-49d8-80a0-b16825419058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2546c3c1-695f-43dd-8b2f-f924a4507005",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    special_tokens=[\n",
    "        (\"[SOS]\", tokenizer.token_to_id(\"[SOS]\")),\n",
    "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12d7808d-e42f-4a44-a99e-bcadafff4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"data/tokenizer-news_commentary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea474c49-6c93-432c-a7c5-ccc61f81ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer.from_file(\"data/tokenizer-news_commentary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05fb95-4541-4bf9-bc61-bd5f54e8dabf",
   "metadata": {},
   "source": [
    "#### Add number of tokens per translation to each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5c4ed6-e338-4046-9bce-b895fbd93a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(row):\n",
    "    row['translation']['count'] = len(tokenizer.encode(row['translation']['ar']).ids) + 1 # Add one because we use [SOS] and [EOS]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d42f450-a7cd-405e-a2ff-e4ef2ff47083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function count_tokens at 0x7f793c1669d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b37147f6a4e4cea8a1bcacf0cf0b1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83187 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_dataset = dataset[0].map(count_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f84113-eb3f-416d-867a-ca72dde2b5d0",
   "metadata": {},
   "source": [
    "We can now get an array containing the number of tokens in the arabic translation per row, and we can use it to create a new id system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e004e7f1-f638-4c51-9449-bc1c30bd891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8,  92,  73,  66, 131,  72, 138,  62,  62,  60])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = np.array(new_dataset.flatten()['translation.count'])\n",
    "counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6e86e4e-e385-4b1b-a557-da251119ed53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   8, 100, 173, 239, 370, 442, 580, 642, 704])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum = np.cumsum(np.pad(counts, (1, 0), 'constant'))\n",
    "cumsum[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "584b7793-15e7-4c82-8f51-d913b0acdbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_map = {}\n",
    "for i, (x1, x2) in enumerate(zip(cumsum[:-1], cumsum[1:])):\n",
    "    for n in np.arange(x1, x2):\n",
    "        idx_map[n] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01f6bc4d-6349-4453-876f-293e83e523c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 0,\n",
       " 7: 0,\n",
       " 8: 1,\n",
       " 9: 1,\n",
       " 10: 1,\n",
       " 11: 1,\n",
       " 12: 1,\n",
       " 13: 1,\n",
       " 14: 1,\n",
       " 15: 1,\n",
       " 16: 1,\n",
       " 17: 1,\n",
       " 18: 1,\n",
       " 19: 1,\n",
       " 20: 1,\n",
       " 21: 1,\n",
       " 22: 1,\n",
       " 23: 1,\n",
       " 24: 1,\n",
       " 25: 1,\n",
       " 26: 1,\n",
       " 27: 1,\n",
       " 28: 1,\n",
       " 29: 1,\n",
       " 30: 1,\n",
       " 31: 1,\n",
       " 32: 1,\n",
       " 33: 1,\n",
       " 34: 1,\n",
       " 35: 1,\n",
       " 36: 1,\n",
       " 37: 1,\n",
       " 38: 1,\n",
       " 39: 1,\n",
       " 40: 1,\n",
       " 41: 1,\n",
       " 42: 1,\n",
       " 43: 1,\n",
       " 44: 1,\n",
       " 45: 1,\n",
       " 46: 1,\n",
       " 47: 1,\n",
       " 48: 1,\n",
       " 49: 1,\n",
       " 50: 1,\n",
       " 51: 1,\n",
       " 52: 1,\n",
       " 53: 1,\n",
       " 54: 1,\n",
       " 55: 1,\n",
       " 56: 1,\n",
       " 57: 1,\n",
       " 58: 1,\n",
       " 59: 1,\n",
       " 60: 1,\n",
       " 61: 1,\n",
       " 62: 1,\n",
       " 63: 1,\n",
       " 64: 1,\n",
       " 65: 1,\n",
       " 66: 1,\n",
       " 67: 1,\n",
       " 68: 1,\n",
       " 69: 1,\n",
       " 70: 1,\n",
       " 71: 1,\n",
       " 72: 1,\n",
       " 73: 1,\n",
       " 74: 1,\n",
       " 75: 1,\n",
       " 76: 1,\n",
       " 77: 1,\n",
       " 78: 1,\n",
       " 79: 1,\n",
       " 80: 1,\n",
       " 81: 1,\n",
       " 82: 1,\n",
       " 83: 1,\n",
       " 84: 1,\n",
       " 85: 1,\n",
       " 86: 1,\n",
       " 87: 1,\n",
       " 88: 1,\n",
       " 89: 1,\n",
       " 90: 1,\n",
       " 91: 1,\n",
       " 92: 1,\n",
       " 93: 1,\n",
       " 94: 1,\n",
       " 95: 1,\n",
       " 96: 1,\n",
       " 97: 1,\n",
       " 98: 1,\n",
       " 99: 1,\n",
       " 100: 2,\n",
       " 101: 2,\n",
       " 102: 2,\n",
       " 103: 2,\n",
       " 104: 2,\n",
       " 105: 2,\n",
       " 106: 2,\n",
       " 107: 2,\n",
       " 108: 2,\n",
       " 109: 2,\n",
       " 110: 2,\n",
       " 111: 2,\n",
       " 112: 2,\n",
       " 113: 2,\n",
       " 114: 2,\n",
       " 115: 2,\n",
       " 116: 2,\n",
       " 117: 2,\n",
       " 118: 2,\n",
       " 119: 2,\n",
       " 120: 2,\n",
       " 121: 2,\n",
       " 122: 2,\n",
       " 123: 2,\n",
       " 124: 2,\n",
       " 125: 2,\n",
       " 126: 2,\n",
       " 127: 2,\n",
       " 128: 2,\n",
       " 129: 2,\n",
       " 130: 2,\n",
       " 131: 2,\n",
       " 132: 2,\n",
       " 133: 2,\n",
       " 134: 2,\n",
       " 135: 2,\n",
       " 136: 2,\n",
       " 137: 2,\n",
       " 138: 2,\n",
       " 139: 2,\n",
       " 140: 2,\n",
       " 141: 2,\n",
       " 142: 2,\n",
       " 143: 2,\n",
       " 144: 2,\n",
       " 145: 2,\n",
       " 146: 2,\n",
       " 147: 2,\n",
       " 148: 2,\n",
       " 149: 2,\n",
       " 150: 2,\n",
       " 151: 2,\n",
       " 152: 2,\n",
       " 153: 2,\n",
       " 154: 2,\n",
       " 155: 2,\n",
       " 156: 2,\n",
       " 157: 2,\n",
       " 158: 2,\n",
       " 159: 2,\n",
       " 160: 2,\n",
       " 161: 2,\n",
       " 162: 2,\n",
       " 163: 2,\n",
       " 164: 2,\n",
       " 165: 2,\n",
       " 166: 2,\n",
       " 167: 2,\n",
       " 168: 2,\n",
       " 169: 2,\n",
       " 170: 2,\n",
       " 171: 2,\n",
       " 172: 2,\n",
       " 173: 3,\n",
       " 174: 3,\n",
       " 175: 3,\n",
       " 176: 3,\n",
       " 177: 3,\n",
       " 178: 3,\n",
       " 179: 3,\n",
       " 180: 3,\n",
       " 181: 3,\n",
       " 182: 3,\n",
       " 183: 3,\n",
       " 184: 3,\n",
       " 185: 3,\n",
       " 186: 3,\n",
       " 187: 3,\n",
       " 188: 3,\n",
       " 189: 3,\n",
       " 190: 3,\n",
       " 191: 3,\n",
       " 192: 3,\n",
       " 193: 3,\n",
       " 194: 3,\n",
       " 195: 3,\n",
       " 196: 3,\n",
       " 197: 3,\n",
       " 198: 3,\n",
       " 199: 3,\n",
       " 200: 3,\n",
       " 201: 3,\n",
       " 202: 3,\n",
       " 203: 3,\n",
       " 204: 3,\n",
       " 205: 3,\n",
       " 206: 3,\n",
       " 207: 3,\n",
       " 208: 3,\n",
       " 209: 3,\n",
       " 210: 3,\n",
       " 211: 3,\n",
       " 212: 3,\n",
       " 213: 3,\n",
       " 214: 3,\n",
       " 215: 3,\n",
       " 216: 3,\n",
       " 217: 3,\n",
       " 218: 3,\n",
       " 219: 3,\n",
       " 220: 3,\n",
       " 221: 3,\n",
       " 222: 3,\n",
       " 223: 3,\n",
       " 224: 3,\n",
       " 225: 3,\n",
       " 226: 3,\n",
       " 227: 3,\n",
       " 228: 3,\n",
       " 229: 3,\n",
       " 230: 3,\n",
       " 231: 3,\n",
       " 232: 3,\n",
       " 233: 3,\n",
       " 234: 3,\n",
       " 235: 3,\n",
       " 236: 3,\n",
       " 237: 3,\n",
       " 238: 3,\n",
       " 239: 4,\n",
       " 240: 4,\n",
       " 241: 4,\n",
       " 242: 4,\n",
       " 243: 4,\n",
       " 244: 4,\n",
       " 245: 4,\n",
       " 246: 4,\n",
       " 247: 4,\n",
       " 248: 4,\n",
       " 249: 4,\n",
       " 250: 4,\n",
       " 251: 4,\n",
       " 252: 4,\n",
       " 253: 4,\n",
       " 254: 4,\n",
       " 255: 4,\n",
       " 256: 4,\n",
       " 257: 4,\n",
       " 258: 4,\n",
       " 259: 4,\n",
       " 260: 4,\n",
       " 261: 4,\n",
       " 262: 4,\n",
       " 263: 4,\n",
       " 264: 4,\n",
       " 265: 4,\n",
       " 266: 4,\n",
       " 267: 4,\n",
       " 268: 4,\n",
       " 269: 4,\n",
       " 270: 4,\n",
       " 271: 4,\n",
       " 272: 4,\n",
       " 273: 4,\n",
       " 274: 4,\n",
       " 275: 4,\n",
       " 276: 4,\n",
       " 277: 4,\n",
       " 278: 4,\n",
       " 279: 4,\n",
       " 280: 4,\n",
       " 281: 4,\n",
       " 282: 4,\n",
       " 283: 4,\n",
       " 284: 4,\n",
       " 285: 4,\n",
       " 286: 4,\n",
       " 287: 4,\n",
       " 288: 4,\n",
       " 289: 4,\n",
       " 290: 4,\n",
       " 291: 4,\n",
       " 292: 4,\n",
       " 293: 4,\n",
       " 294: 4,\n",
       " 295: 4,\n",
       " 296: 4,\n",
       " 297: 4,\n",
       " 298: 4,\n",
       " 299: 4,\n",
       " 300: 4,\n",
       " 301: 4,\n",
       " 302: 4,\n",
       " 303: 4,\n",
       " 304: 4,\n",
       " 305: 4,\n",
       " 306: 4,\n",
       " 307: 4,\n",
       " 308: 4,\n",
       " 309: 4,\n",
       " 310: 4,\n",
       " 311: 4,\n",
       " 312: 4,\n",
       " 313: 4,\n",
       " 314: 4,\n",
       " 315: 4,\n",
       " 316: 4,\n",
       " 317: 4,\n",
       " 318: 4,\n",
       " 319: 4,\n",
       " 320: 4,\n",
       " 321: 4,\n",
       " 322: 4,\n",
       " 323: 4,\n",
       " 324: 4,\n",
       " 325: 4,\n",
       " 326: 4,\n",
       " 327: 4,\n",
       " 328: 4,\n",
       " 329: 4,\n",
       " 330: 4,\n",
       " 331: 4,\n",
       " 332: 4,\n",
       " 333: 4,\n",
       " 334: 4,\n",
       " 335: 4,\n",
       " 336: 4,\n",
       " 337: 4,\n",
       " 338: 4,\n",
       " 339: 4,\n",
       " 340: 4,\n",
       " 341: 4,\n",
       " 342: 4,\n",
       " 343: 4,\n",
       " 344: 4,\n",
       " 345: 4,\n",
       " 346: 4,\n",
       " 347: 4,\n",
       " 348: 4,\n",
       " 349: 4,\n",
       " 350: 4,\n",
       " 351: 4,\n",
       " 352: 4,\n",
       " 353: 4,\n",
       " 354: 4,\n",
       " 355: 4,\n",
       " 356: 4,\n",
       " 357: 4,\n",
       " 358: 4,\n",
       " 359: 4,\n",
       " 360: 4,\n",
       " 361: 4,\n",
       " 362: 4,\n",
       " 363: 4,\n",
       " 364: 4,\n",
       " 365: 4,\n",
       " 366: 4,\n",
       " 367: 4,\n",
       " 368: 4,\n",
       " 369: 4,\n",
       " 370: 5,\n",
       " 371: 5,\n",
       " 372: 5,\n",
       " 373: 5,\n",
       " 374: 5,\n",
       " 375: 5,\n",
       " 376: 5,\n",
       " 377: 5,\n",
       " 378: 5,\n",
       " 379: 5,\n",
       " 380: 5,\n",
       " 381: 5,\n",
       " 382: 5,\n",
       " 383: 5,\n",
       " 384: 5,\n",
       " 385: 5,\n",
       " 386: 5,\n",
       " 387: 5,\n",
       " 388: 5,\n",
       " 389: 5,\n",
       " 390: 5,\n",
       " 391: 5,\n",
       " 392: 5,\n",
       " 393: 5,\n",
       " 394: 5,\n",
       " 395: 5,\n",
       " 396: 5,\n",
       " 397: 5,\n",
       " 398: 5,\n",
       " 399: 5,\n",
       " 400: 5,\n",
       " 401: 5,\n",
       " 402: 5,\n",
       " 403: 5,\n",
       " 404: 5,\n",
       " 405: 5,\n",
       " 406: 5,\n",
       " 407: 5,\n",
       " 408: 5,\n",
       " 409: 5,\n",
       " 410: 5,\n",
       " 411: 5,\n",
       " 412: 5,\n",
       " 413: 5,\n",
       " 414: 5,\n",
       " 415: 5,\n",
       " 416: 5,\n",
       " 417: 5,\n",
       " 418: 5,\n",
       " 419: 5,\n",
       " 420: 5,\n",
       " 421: 5,\n",
       " 422: 5,\n",
       " 423: 5,\n",
       " 424: 5,\n",
       " 425: 5,\n",
       " 426: 5,\n",
       " 427: 5,\n",
       " 428: 5,\n",
       " 429: 5,\n",
       " 430: 5,\n",
       " 431: 5,\n",
       " 432: 5,\n",
       " 433: 5,\n",
       " 434: 5,\n",
       " 435: 5,\n",
       " 436: 5,\n",
       " 437: 5,\n",
       " 438: 5,\n",
       " 439: 5,\n",
       " 440: 5,\n",
       " 441: 5,\n",
       " 442: 6,\n",
       " 443: 6,\n",
       " 444: 6,\n",
       " 445: 6,\n",
       " 446: 6,\n",
       " 447: 6,\n",
       " 448: 6,\n",
       " 449: 6,\n",
       " 450: 6,\n",
       " 451: 6,\n",
       " 452: 6,\n",
       " 453: 6,\n",
       " 454: 6,\n",
       " 455: 6,\n",
       " 456: 6,\n",
       " 457: 6,\n",
       " 458: 6,\n",
       " 459: 6,\n",
       " 460: 6,\n",
       " 461: 6,\n",
       " 462: 6,\n",
       " 463: 6,\n",
       " 464: 6,\n",
       " 465: 6,\n",
       " 466: 6,\n",
       " 467: 6,\n",
       " 468: 6,\n",
       " 469: 6,\n",
       " 470: 6,\n",
       " 471: 6,\n",
       " 472: 6,\n",
       " 473: 6,\n",
       " 474: 6,\n",
       " 475: 6,\n",
       " 476: 6,\n",
       " 477: 6,\n",
       " 478: 6,\n",
       " 479: 6,\n",
       " 480: 6,\n",
       " 481: 6,\n",
       " 482: 6,\n",
       " 483: 6,\n",
       " 484: 6,\n",
       " 485: 6,\n",
       " 486: 6,\n",
       " 487: 6,\n",
       " 488: 6,\n",
       " 489: 6,\n",
       " 490: 6,\n",
       " 491: 6,\n",
       " 492: 6,\n",
       " 493: 6,\n",
       " 494: 6,\n",
       " 495: 6,\n",
       " 496: 6,\n",
       " 497: 6,\n",
       " 498: 6,\n",
       " 499: 6,\n",
       " 500: 6,\n",
       " 501: 6,\n",
       " 502: 6,\n",
       " 503: 6,\n",
       " 504: 6,\n",
       " 505: 6,\n",
       " 506: 6,\n",
       " 507: 6,\n",
       " 508: 6,\n",
       " 509: 6,\n",
       " 510: 6,\n",
       " 511: 6,\n",
       " 512: 6,\n",
       " 513: 6,\n",
       " 514: 6,\n",
       " 515: 6,\n",
       " 516: 6,\n",
       " 517: 6,\n",
       " 518: 6,\n",
       " 519: 6,\n",
       " 520: 6,\n",
       " 521: 6,\n",
       " 522: 6,\n",
       " 523: 6,\n",
       " 524: 6,\n",
       " 525: 6,\n",
       " 526: 6,\n",
       " 527: 6,\n",
       " 528: 6,\n",
       " 529: 6,\n",
       " 530: 6,\n",
       " 531: 6,\n",
       " 532: 6,\n",
       " 533: 6,\n",
       " 534: 6,\n",
       " 535: 6,\n",
       " 536: 6,\n",
       " 537: 6,\n",
       " 538: 6,\n",
       " 539: 6,\n",
       " 540: 6,\n",
       " 541: 6,\n",
       " 542: 6,\n",
       " 543: 6,\n",
       " 544: 6,\n",
       " 545: 6,\n",
       " 546: 6,\n",
       " 547: 6,\n",
       " 548: 6,\n",
       " 549: 6,\n",
       " 550: 6,\n",
       " 551: 6,\n",
       " 552: 6,\n",
       " 553: 6,\n",
       " 554: 6,\n",
       " 555: 6,\n",
       " 556: 6,\n",
       " 557: 6,\n",
       " 558: 6,\n",
       " 559: 6,\n",
       " 560: 6,\n",
       " 561: 6,\n",
       " 562: 6,\n",
       " 563: 6,\n",
       " 564: 6,\n",
       " 565: 6,\n",
       " 566: 6,\n",
       " 567: 6,\n",
       " 568: 6,\n",
       " 569: 6,\n",
       " 570: 6,\n",
       " 571: 6,\n",
       " 572: 6,\n",
       " 573: 6,\n",
       " 574: 6,\n",
       " 575: 6,\n",
       " 576: 6,\n",
       " 577: 6,\n",
       " 578: 6,\n",
       " 579: 6,\n",
       " 580: 7,\n",
       " 581: 7,\n",
       " 582: 7,\n",
       " 583: 7,\n",
       " 584: 7,\n",
       " 585: 7,\n",
       " 586: 7,\n",
       " 587: 7,\n",
       " 588: 7,\n",
       " 589: 7,\n",
       " 590: 7,\n",
       " 591: 7,\n",
       " 592: 7,\n",
       " 593: 7,\n",
       " 594: 7,\n",
       " 595: 7,\n",
       " 596: 7,\n",
       " 597: 7,\n",
       " 598: 7,\n",
       " 599: 7,\n",
       " 600: 7,\n",
       " 601: 7,\n",
       " 602: 7,\n",
       " 603: 7,\n",
       " 604: 7,\n",
       " 605: 7,\n",
       " 606: 7,\n",
       " 607: 7,\n",
       " 608: 7,\n",
       " 609: 7,\n",
       " 610: 7,\n",
       " 611: 7,\n",
       " 612: 7,\n",
       " 613: 7,\n",
       " 614: 7,\n",
       " 615: 7,\n",
       " 616: 7,\n",
       " 617: 7,\n",
       " 618: 7,\n",
       " 619: 7,\n",
       " 620: 7,\n",
       " 621: 7,\n",
       " 622: 7,\n",
       " 623: 7,\n",
       " 624: 7,\n",
       " 625: 7,\n",
       " 626: 7,\n",
       " 627: 7,\n",
       " 628: 7,\n",
       " 629: 7,\n",
       " 630: 7,\n",
       " 631: 7,\n",
       " 632: 7,\n",
       " 633: 7,\n",
       " 634: 7,\n",
       " 635: 7,\n",
       " 636: 7,\n",
       " 637: 7,\n",
       " 638: 7,\n",
       " 639: 7,\n",
       " 640: 7,\n",
       " 641: 7,\n",
       " 642: 8,\n",
       " 643: 8,\n",
       " 644: 8,\n",
       " 645: 8,\n",
       " 646: 8,\n",
       " 647: 8,\n",
       " 648: 8,\n",
       " 649: 8,\n",
       " 650: 8,\n",
       " 651: 8,\n",
       " 652: 8,\n",
       " 653: 8,\n",
       " 654: 8,\n",
       " 655: 8,\n",
       " 656: 8,\n",
       " 657: 8,\n",
       " 658: 8,\n",
       " 659: 8,\n",
       " 660: 8,\n",
       " 661: 8,\n",
       " 662: 8,\n",
       " 663: 8,\n",
       " 664: 8,\n",
       " 665: 8,\n",
       " 666: 8,\n",
       " 667: 8,\n",
       " 668: 8,\n",
       " 669: 8,\n",
       " 670: 8,\n",
       " 671: 8,\n",
       " 672: 8,\n",
       " 673: 8,\n",
       " 674: 8,\n",
       " 675: 8,\n",
       " 676: 8,\n",
       " 677: 8,\n",
       " 678: 8,\n",
       " 679: 8,\n",
       " 680: 8,\n",
       " 681: 8,\n",
       " 682: 8,\n",
       " 683: 8,\n",
       " 684: 8,\n",
       " 685: 8,\n",
       " 686: 8,\n",
       " 687: 8,\n",
       " 688: 8,\n",
       " 689: 8,\n",
       " 690: 8,\n",
       " 691: 8,\n",
       " 692: 8,\n",
       " 693: 8,\n",
       " 694: 8,\n",
       " 695: 8,\n",
       " 696: 8,\n",
       " 697: 8,\n",
       " 698: 8,\n",
       " 699: 8,\n",
       " 700: 8,\n",
       " 701: 8,\n",
       " 702: 8,\n",
       " 703: 8,\n",
       " 704: 9,\n",
       " 705: 9,\n",
       " 706: 9,\n",
       " 707: 9,\n",
       " 708: 9,\n",
       " 709: 9,\n",
       " 710: 9,\n",
       " 711: 9,\n",
       " 712: 9,\n",
       " 713: 9,\n",
       " 714: 9,\n",
       " 715: 9,\n",
       " 716: 9,\n",
       " 717: 9,\n",
       " 718: 9,\n",
       " 719: 9,\n",
       " 720: 9,\n",
       " 721: 9,\n",
       " 722: 9,\n",
       " 723: 9,\n",
       " 724: 9,\n",
       " 725: 9,\n",
       " 726: 9,\n",
       " 727: 9,\n",
       " 728: 9,\n",
       " 729: 9,\n",
       " 730: 9,\n",
       " 731: 9,\n",
       " 732: 9,\n",
       " 733: 9,\n",
       " 734: 9,\n",
       " 735: 9,\n",
       " 736: 9,\n",
       " 737: 9,\n",
       " 738: 9,\n",
       " 739: 9,\n",
       " 740: 9,\n",
       " 741: 9,\n",
       " 742: 9,\n",
       " 743: 9,\n",
       " 744: 9,\n",
       " 745: 9,\n",
       " 746: 9,\n",
       " 747: 9,\n",
       " 748: 9,\n",
       " 749: 9,\n",
       " 750: 9,\n",
       " 751: 9,\n",
       " 752: 9,\n",
       " 753: 9,\n",
       " 754: 9,\n",
       " 755: 9,\n",
       " 756: 9,\n",
       " 757: 9,\n",
       " 758: 9,\n",
       " 759: 9,\n",
       " 760: 9,\n",
       " 761: 9,\n",
       " 762: 9,\n",
       " 763: 9,\n",
       " 764: 10,\n",
       " 765: 10,\n",
       " 766: 10,\n",
       " 767: 10,\n",
       " 768: 10,\n",
       " 769: 10,\n",
       " 770: 10,\n",
       " 771: 10,\n",
       " 772: 10,\n",
       " 773: 10,\n",
       " 774: 10,\n",
       " 775: 10,\n",
       " 776: 10,\n",
       " 777: 10,\n",
       " 778: 10,\n",
       " 779: 10,\n",
       " 780: 10,\n",
       " 781: 10,\n",
       " 782: 10,\n",
       " 783: 10,\n",
       " 784: 10,\n",
       " 785: 10,\n",
       " 786: 10,\n",
       " 787: 10,\n",
       " 788: 10,\n",
       " 789: 10,\n",
       " 790: 10,\n",
       " 791: 10,\n",
       " 792: 10,\n",
       " 793: 10,\n",
       " 794: 10,\n",
       " 795: 10,\n",
       " 796: 10,\n",
       " 797: 10,\n",
       " 798: 10,\n",
       " 799: 10,\n",
       " 800: 10,\n",
       " 801: 10,\n",
       " 802: 10,\n",
       " 803: 10,\n",
       " 804: 10,\n",
       " 805: 10,\n",
       " 806: 10,\n",
       " 807: 10,\n",
       " 808: 10,\n",
       " 809: 10,\n",
       " 810: 10,\n",
       " 811: 10,\n",
       " 812: 10,\n",
       " 813: 10,\n",
       " 814: 10,\n",
       " 815: 10,\n",
       " 816: 10,\n",
       " 817: 10,\n",
       " 818: 10,\n",
       " 819: 10,\n",
       " 820: 11,\n",
       " 821: 11,\n",
       " 822: 11,\n",
       " 823: 11,\n",
       " 824: 11,\n",
       " 825: 11,\n",
       " 826: 11,\n",
       " 827: 11,\n",
       " 828: 11,\n",
       " 829: 11,\n",
       " 830: 11,\n",
       " 831: 11,\n",
       " 832: 11,\n",
       " 833: 11,\n",
       " 834: 11,\n",
       " 835: 11,\n",
       " 836: 11,\n",
       " 837: 11,\n",
       " 838: 11,\n",
       " 839: 11,\n",
       " 840: 11,\n",
       " 841: 11,\n",
       " 842: 11,\n",
       " 843: 11,\n",
       " 844: 11,\n",
       " 845: 11,\n",
       " 846: 11,\n",
       " 847: 11,\n",
       " 848: 11,\n",
       " 849: 11,\n",
       " 850: 11,\n",
       " 851: 11,\n",
       " 852: 11,\n",
       " 853: 11,\n",
       " 854: 11,\n",
       " 855: 11,\n",
       " 856: 11,\n",
       " 857: 11,\n",
       " 858: 11,\n",
       " 859: 11,\n",
       " 860: 11,\n",
       " 861: 11,\n",
       " 862: 11,\n",
       " 863: 11,\n",
       " 864: 12,\n",
       " 865: 12,\n",
       " 866: 12,\n",
       " 867: 12,\n",
       " 868: 12,\n",
       " 869: 12,\n",
       " 870: 12,\n",
       " 871: 12,\n",
       " 872: 12,\n",
       " 873: 12,\n",
       " 874: 12,\n",
       " 875: 12,\n",
       " 876: 12,\n",
       " 877: 12,\n",
       " 878: 12,\n",
       " 879: 12,\n",
       " 880: 12,\n",
       " 881: 12,\n",
       " 882: 12,\n",
       " 883: 12,\n",
       " 884: 12,\n",
       " 885: 12,\n",
       " 886: 12,\n",
       " 887: 12,\n",
       " 888: 12,\n",
       " 889: 12,\n",
       " 890: 12,\n",
       " 891: 12,\n",
       " 892: 12,\n",
       " 893: 12,\n",
       " 894: 12,\n",
       " 895: 12,\n",
       " 896: 12,\n",
       " 897: 12,\n",
       " 898: 12,\n",
       " 899: 12,\n",
       " 900: 12,\n",
       " 901: 12,\n",
       " 902: 12,\n",
       " 903: 12,\n",
       " 904: 12,\n",
       " 905: 12,\n",
       " 906: 12,\n",
       " 907: 12,\n",
       " 908: 12,\n",
       " 909: 12,\n",
       " 910: 12,\n",
       " 911: 12,\n",
       " 912: 12,\n",
       " 913: 12,\n",
       " 914: 12,\n",
       " 915: 12,\n",
       " 916: 12,\n",
       " 917: 12,\n",
       " 918: 12,\n",
       " 919: 12,\n",
       " 920: 12,\n",
       " 921: 12,\n",
       " 922: 12,\n",
       " 923: 12,\n",
       " 924: 12,\n",
       " 925: 12,\n",
       " 926: 12,\n",
       " 927: 12,\n",
       " 928: 12,\n",
       " 929: 12,\n",
       " 930: 12,\n",
       " 931: 12,\n",
       " 932: 12,\n",
       " 933: 12,\n",
       " 934: 12,\n",
       " 935: 12,\n",
       " 936: 12,\n",
       " 937: 12,\n",
       " 938: 12,\n",
       " 939: 12,\n",
       " 940: 12,\n",
       " 941: 12,\n",
       " 942: 12,\n",
       " 943: 12,\n",
       " 944: 12,\n",
       " 945: 12,\n",
       " 946: 12,\n",
       " 947: 12,\n",
       " 948: 12,\n",
       " 949: 12,\n",
       " 950: 12,\n",
       " 951: 12,\n",
       " 952: 12,\n",
       " 953: 12,\n",
       " 954: 12,\n",
       " 955: 12,\n",
       " 956: 12,\n",
       " 957: 12,\n",
       " 958: 12,\n",
       " 959: 12,\n",
       " 960: 12,\n",
       " 961: 12,\n",
       " 962: 12,\n",
       " 963: 12,\n",
       " 964: 12,\n",
       " 965: 12,\n",
       " 966: 12,\n",
       " 967: 12,\n",
       " 968: 12,\n",
       " 969: 12,\n",
       " 970: 12,\n",
       " 971: 12,\n",
       " 972: 13,\n",
       " 973: 13,\n",
       " 974: 13,\n",
       " 975: 13,\n",
       " 976: 13,\n",
       " 977: 13,\n",
       " 978: 13,\n",
       " 979: 13,\n",
       " 980: 13,\n",
       " 981: 13,\n",
       " 982: 13,\n",
       " 983: 13,\n",
       " 984: 13,\n",
       " 985: 13,\n",
       " 986: 13,\n",
       " 987: 13,\n",
       " 988: 13,\n",
       " 989: 13,\n",
       " 990: 13,\n",
       " 991: 13,\n",
       " 992: 13,\n",
       " 993: 13,\n",
       " 994: 13,\n",
       " 995: 13,\n",
       " 996: 13,\n",
       " 997: 13,\n",
       " 998: 13,\n",
       " 999: 13,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffd092-af27-4fbe-9aa1-3ee35434ccfe",
   "metadata": {},
   "source": [
    "Now we have a dict that maps the new id to the original dataset id, but we need a way to figure out which is the cutoff token to process in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae3aa2e2-844a-4750-9d6d-c7a71150792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_map = {}\n",
    "for x1, x2 in zip(cumsum[:-1], cumsum[1:]):\n",
    "    for i, n in enumerate(np.arange(x1, x2)):\n",
    "        token_map[n] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d3f9374-f497-400c-beeb-c14309e675c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 0,\n",
       " 9: 1,\n",
       " 10: 2,\n",
       " 11: 3,\n",
       " 12: 4,\n",
       " 13: 5,\n",
       " 14: 6,\n",
       " 15: 7,\n",
       " 16: 8,\n",
       " 17: 9,\n",
       " 18: 10,\n",
       " 19: 11,\n",
       " 20: 12,\n",
       " 21: 13,\n",
       " 22: 14,\n",
       " 23: 15,\n",
       " 24: 16,\n",
       " 25: 17,\n",
       " 26: 18,\n",
       " 27: 19,\n",
       " 28: 20,\n",
       " 29: 21,\n",
       " 30: 22,\n",
       " 31: 23,\n",
       " 32: 24,\n",
       " 33: 25,\n",
       " 34: 26,\n",
       " 35: 27,\n",
       " 36: 28,\n",
       " 37: 29,\n",
       " 38: 30,\n",
       " 39: 31,\n",
       " 40: 32,\n",
       " 41: 33,\n",
       " 42: 34,\n",
       " 43: 35,\n",
       " 44: 36,\n",
       " 45: 37,\n",
       " 46: 38,\n",
       " 47: 39,\n",
       " 48: 40,\n",
       " 49: 41,\n",
       " 50: 42,\n",
       " 51: 43,\n",
       " 52: 44,\n",
       " 53: 45,\n",
       " 54: 46,\n",
       " 55: 47,\n",
       " 56: 48,\n",
       " 57: 49,\n",
       " 58: 50,\n",
       " 59: 51,\n",
       " 60: 52,\n",
       " 61: 53,\n",
       " 62: 54,\n",
       " 63: 55,\n",
       " 64: 56,\n",
       " 65: 57,\n",
       " 66: 58,\n",
       " 67: 59,\n",
       " 68: 60,\n",
       " 69: 61,\n",
       " 70: 62,\n",
       " 71: 63,\n",
       " 72: 64,\n",
       " 73: 65,\n",
       " 74: 66,\n",
       " 75: 67,\n",
       " 76: 68,\n",
       " 77: 69,\n",
       " 78: 70,\n",
       " 79: 71,\n",
       " 80: 72,\n",
       " 81: 73,\n",
       " 82: 74,\n",
       " 83: 75,\n",
       " 84: 76,\n",
       " 85: 77,\n",
       " 86: 78,\n",
       " 87: 79,\n",
       " 88: 80,\n",
       " 89: 81,\n",
       " 90: 82,\n",
       " 91: 83,\n",
       " 92: 84,\n",
       " 93: 85,\n",
       " 94: 86,\n",
       " 95: 87,\n",
       " 96: 88,\n",
       " 97: 89,\n",
       " 98: 90,\n",
       " 99: 91,\n",
       " 100: 0,\n",
       " 101: 1,\n",
       " 102: 2,\n",
       " 103: 3,\n",
       " 104: 4,\n",
       " 105: 5,\n",
       " 106: 6,\n",
       " 107: 7,\n",
       " 108: 8,\n",
       " 109: 9,\n",
       " 110: 10,\n",
       " 111: 11,\n",
       " 112: 12,\n",
       " 113: 13,\n",
       " 114: 14,\n",
       " 115: 15,\n",
       " 116: 16,\n",
       " 117: 17,\n",
       " 118: 18,\n",
       " 119: 19,\n",
       " 120: 20,\n",
       " 121: 21,\n",
       " 122: 22,\n",
       " 123: 23,\n",
       " 124: 24,\n",
       " 125: 25,\n",
       " 126: 26,\n",
       " 127: 27,\n",
       " 128: 28,\n",
       " 129: 29,\n",
       " 130: 30,\n",
       " 131: 31,\n",
       " 132: 32,\n",
       " 133: 33,\n",
       " 134: 34,\n",
       " 135: 35,\n",
       " 136: 36,\n",
       " 137: 37,\n",
       " 138: 38,\n",
       " 139: 39,\n",
       " 140: 40,\n",
       " 141: 41,\n",
       " 142: 42,\n",
       " 143: 43,\n",
       " 144: 44,\n",
       " 145: 45,\n",
       " 146: 46,\n",
       " 147: 47,\n",
       " 148: 48,\n",
       " 149: 49,\n",
       " 150: 50,\n",
       " 151: 51,\n",
       " 152: 52,\n",
       " 153: 53,\n",
       " 154: 54,\n",
       " 155: 55,\n",
       " 156: 56,\n",
       " 157: 57,\n",
       " 158: 58,\n",
       " 159: 59,\n",
       " 160: 60,\n",
       " 161: 61,\n",
       " 162: 62,\n",
       " 163: 63,\n",
       " 164: 64,\n",
       " 165: 65,\n",
       " 166: 66,\n",
       " 167: 67,\n",
       " 168: 68,\n",
       " 169: 69,\n",
       " 170: 70,\n",
       " 171: 71,\n",
       " 172: 72,\n",
       " 173: 0,\n",
       " 174: 1,\n",
       " 175: 2,\n",
       " 176: 3,\n",
       " 177: 4,\n",
       " 178: 5,\n",
       " 179: 6,\n",
       " 180: 7,\n",
       " 181: 8,\n",
       " 182: 9,\n",
       " 183: 10,\n",
       " 184: 11,\n",
       " 185: 12,\n",
       " 186: 13,\n",
       " 187: 14,\n",
       " 188: 15,\n",
       " 189: 16,\n",
       " 190: 17,\n",
       " 191: 18,\n",
       " 192: 19,\n",
       " 193: 20,\n",
       " 194: 21,\n",
       " 195: 22,\n",
       " 196: 23,\n",
       " 197: 24,\n",
       " 198: 25,\n",
       " 199: 26,\n",
       " 200: 27,\n",
       " 201: 28,\n",
       " 202: 29,\n",
       " 203: 30,\n",
       " 204: 31,\n",
       " 205: 32,\n",
       " 206: 33,\n",
       " 207: 34,\n",
       " 208: 35,\n",
       " 209: 36,\n",
       " 210: 37,\n",
       " 211: 38,\n",
       " 212: 39,\n",
       " 213: 40,\n",
       " 214: 41,\n",
       " 215: 42,\n",
       " 216: 43,\n",
       " 217: 44,\n",
       " 218: 45,\n",
       " 219: 46,\n",
       " 220: 47,\n",
       " 221: 48,\n",
       " 222: 49,\n",
       " 223: 50,\n",
       " 224: 51,\n",
       " 225: 52,\n",
       " 226: 53,\n",
       " 227: 54,\n",
       " 228: 55,\n",
       " 229: 56,\n",
       " 230: 57,\n",
       " 231: 58,\n",
       " 232: 59,\n",
       " 233: 60,\n",
       " 234: 61,\n",
       " 235: 62,\n",
       " 236: 63,\n",
       " 237: 64,\n",
       " 238: 65,\n",
       " 239: 0,\n",
       " 240: 1,\n",
       " 241: 2,\n",
       " 242: 3,\n",
       " 243: 4,\n",
       " 244: 5,\n",
       " 245: 6,\n",
       " 246: 7,\n",
       " 247: 8,\n",
       " 248: 9,\n",
       " 249: 10,\n",
       " 250: 11,\n",
       " 251: 12,\n",
       " 252: 13,\n",
       " 253: 14,\n",
       " 254: 15,\n",
       " 255: 16,\n",
       " 256: 17,\n",
       " 257: 18,\n",
       " 258: 19,\n",
       " 259: 20,\n",
       " 260: 21,\n",
       " 261: 22,\n",
       " 262: 23,\n",
       " 263: 24,\n",
       " 264: 25,\n",
       " 265: 26,\n",
       " 266: 27,\n",
       " 267: 28,\n",
       " 268: 29,\n",
       " 269: 30,\n",
       " 270: 31,\n",
       " 271: 32,\n",
       " 272: 33,\n",
       " 273: 34,\n",
       " 274: 35,\n",
       " 275: 36,\n",
       " 276: 37,\n",
       " 277: 38,\n",
       " 278: 39,\n",
       " 279: 40,\n",
       " 280: 41,\n",
       " 281: 42,\n",
       " 282: 43,\n",
       " 283: 44,\n",
       " 284: 45,\n",
       " 285: 46,\n",
       " 286: 47,\n",
       " 287: 48,\n",
       " 288: 49,\n",
       " 289: 50,\n",
       " 290: 51,\n",
       " 291: 52,\n",
       " 292: 53,\n",
       " 293: 54,\n",
       " 294: 55,\n",
       " 295: 56,\n",
       " 296: 57,\n",
       " 297: 58,\n",
       " 298: 59,\n",
       " 299: 60,\n",
       " 300: 61,\n",
       " 301: 62,\n",
       " 302: 63,\n",
       " 303: 64,\n",
       " 304: 65,\n",
       " 305: 66,\n",
       " 306: 67,\n",
       " 307: 68,\n",
       " 308: 69,\n",
       " 309: 70,\n",
       " 310: 71,\n",
       " 311: 72,\n",
       " 312: 73,\n",
       " 313: 74,\n",
       " 314: 75,\n",
       " 315: 76,\n",
       " 316: 77,\n",
       " 317: 78,\n",
       " 318: 79,\n",
       " 319: 80,\n",
       " 320: 81,\n",
       " 321: 82,\n",
       " 322: 83,\n",
       " 323: 84,\n",
       " 324: 85,\n",
       " 325: 86,\n",
       " 326: 87,\n",
       " 327: 88,\n",
       " 328: 89,\n",
       " 329: 90,\n",
       " 330: 91,\n",
       " 331: 92,\n",
       " 332: 93,\n",
       " 333: 94,\n",
       " 334: 95,\n",
       " 335: 96,\n",
       " 336: 97,\n",
       " 337: 98,\n",
       " 338: 99,\n",
       " 339: 100,\n",
       " 340: 101,\n",
       " 341: 102,\n",
       " 342: 103,\n",
       " 343: 104,\n",
       " 344: 105,\n",
       " 345: 106,\n",
       " 346: 107,\n",
       " 347: 108,\n",
       " 348: 109,\n",
       " 349: 110,\n",
       " 350: 111,\n",
       " 351: 112,\n",
       " 352: 113,\n",
       " 353: 114,\n",
       " 354: 115,\n",
       " 355: 116,\n",
       " 356: 117,\n",
       " 357: 118,\n",
       " 358: 119,\n",
       " 359: 120,\n",
       " 360: 121,\n",
       " 361: 122,\n",
       " 362: 123,\n",
       " 363: 124,\n",
       " 364: 125,\n",
       " 365: 126,\n",
       " 366: 127,\n",
       " 367: 128,\n",
       " 368: 129,\n",
       " 369: 130,\n",
       " 370: 0,\n",
       " 371: 1,\n",
       " 372: 2,\n",
       " 373: 3,\n",
       " 374: 4,\n",
       " 375: 5,\n",
       " 376: 6,\n",
       " 377: 7,\n",
       " 378: 8,\n",
       " 379: 9,\n",
       " 380: 10,\n",
       " 381: 11,\n",
       " 382: 12,\n",
       " 383: 13,\n",
       " 384: 14,\n",
       " 385: 15,\n",
       " 386: 16,\n",
       " 387: 17,\n",
       " 388: 18,\n",
       " 389: 19,\n",
       " 390: 20,\n",
       " 391: 21,\n",
       " 392: 22,\n",
       " 393: 23,\n",
       " 394: 24,\n",
       " 395: 25,\n",
       " 396: 26,\n",
       " 397: 27,\n",
       " 398: 28,\n",
       " 399: 29,\n",
       " 400: 30,\n",
       " 401: 31,\n",
       " 402: 32,\n",
       " 403: 33,\n",
       " 404: 34,\n",
       " 405: 35,\n",
       " 406: 36,\n",
       " 407: 37,\n",
       " 408: 38,\n",
       " 409: 39,\n",
       " 410: 40,\n",
       " 411: 41,\n",
       " 412: 42,\n",
       " 413: 43,\n",
       " 414: 44,\n",
       " 415: 45,\n",
       " 416: 46,\n",
       " 417: 47,\n",
       " 418: 48,\n",
       " 419: 49,\n",
       " 420: 50,\n",
       " 421: 51,\n",
       " 422: 52,\n",
       " 423: 53,\n",
       " 424: 54,\n",
       " 425: 55,\n",
       " 426: 56,\n",
       " 427: 57,\n",
       " 428: 58,\n",
       " 429: 59,\n",
       " 430: 60,\n",
       " 431: 61,\n",
       " 432: 62,\n",
       " 433: 63,\n",
       " 434: 64,\n",
       " 435: 65,\n",
       " 436: 66,\n",
       " 437: 67,\n",
       " 438: 68,\n",
       " 439: 69,\n",
       " 440: 70,\n",
       " 441: 71,\n",
       " 442: 0,\n",
       " 443: 1,\n",
       " 444: 2,\n",
       " 445: 3,\n",
       " 446: 4,\n",
       " 447: 5,\n",
       " 448: 6,\n",
       " 449: 7,\n",
       " 450: 8,\n",
       " 451: 9,\n",
       " 452: 10,\n",
       " 453: 11,\n",
       " 454: 12,\n",
       " 455: 13,\n",
       " 456: 14,\n",
       " 457: 15,\n",
       " 458: 16,\n",
       " 459: 17,\n",
       " 460: 18,\n",
       " 461: 19,\n",
       " 462: 20,\n",
       " 463: 21,\n",
       " 464: 22,\n",
       " 465: 23,\n",
       " 466: 24,\n",
       " 467: 25,\n",
       " 468: 26,\n",
       " 469: 27,\n",
       " 470: 28,\n",
       " 471: 29,\n",
       " 472: 30,\n",
       " 473: 31,\n",
       " 474: 32,\n",
       " 475: 33,\n",
       " 476: 34,\n",
       " 477: 35,\n",
       " 478: 36,\n",
       " 479: 37,\n",
       " 480: 38,\n",
       " 481: 39,\n",
       " 482: 40,\n",
       " 483: 41,\n",
       " 484: 42,\n",
       " 485: 43,\n",
       " 486: 44,\n",
       " 487: 45,\n",
       " 488: 46,\n",
       " 489: 47,\n",
       " 490: 48,\n",
       " 491: 49,\n",
       " 492: 50,\n",
       " 493: 51,\n",
       " 494: 52,\n",
       " 495: 53,\n",
       " 496: 54,\n",
       " 497: 55,\n",
       " 498: 56,\n",
       " 499: 57,\n",
       " 500: 58,\n",
       " 501: 59,\n",
       " 502: 60,\n",
       " 503: 61,\n",
       " 504: 62,\n",
       " 505: 63,\n",
       " 506: 64,\n",
       " 507: 65,\n",
       " 508: 66,\n",
       " 509: 67,\n",
       " 510: 68,\n",
       " 511: 69,\n",
       " 512: 70,\n",
       " 513: 71,\n",
       " 514: 72,\n",
       " 515: 73,\n",
       " 516: 74,\n",
       " 517: 75,\n",
       " 518: 76,\n",
       " 519: 77,\n",
       " 520: 78,\n",
       " 521: 79,\n",
       " 522: 80,\n",
       " 523: 81,\n",
       " 524: 82,\n",
       " 525: 83,\n",
       " 526: 84,\n",
       " 527: 85,\n",
       " 528: 86,\n",
       " 529: 87,\n",
       " 530: 88,\n",
       " 531: 89,\n",
       " 532: 90,\n",
       " 533: 91,\n",
       " 534: 92,\n",
       " 535: 93,\n",
       " 536: 94,\n",
       " 537: 95,\n",
       " 538: 96,\n",
       " 539: 97,\n",
       " 540: 98,\n",
       " 541: 99,\n",
       " 542: 100,\n",
       " 543: 101,\n",
       " 544: 102,\n",
       " 545: 103,\n",
       " 546: 104,\n",
       " 547: 105,\n",
       " 548: 106,\n",
       " 549: 107,\n",
       " 550: 108,\n",
       " 551: 109,\n",
       " 552: 110,\n",
       " 553: 111,\n",
       " 554: 112,\n",
       " 555: 113,\n",
       " 556: 114,\n",
       " 557: 115,\n",
       " 558: 116,\n",
       " 559: 117,\n",
       " 560: 118,\n",
       " 561: 119,\n",
       " 562: 120,\n",
       " 563: 121,\n",
       " 564: 122,\n",
       " 565: 123,\n",
       " 566: 124,\n",
       " 567: 125,\n",
       " 568: 126,\n",
       " 569: 127,\n",
       " 570: 128,\n",
       " 571: 129,\n",
       " 572: 130,\n",
       " 573: 131,\n",
       " 574: 132,\n",
       " 575: 133,\n",
       " 576: 134,\n",
       " 577: 135,\n",
       " 578: 136,\n",
       " 579: 137,\n",
       " 580: 0,\n",
       " 581: 1,\n",
       " 582: 2,\n",
       " 583: 3,\n",
       " 584: 4,\n",
       " 585: 5,\n",
       " 586: 6,\n",
       " 587: 7,\n",
       " 588: 8,\n",
       " 589: 9,\n",
       " 590: 10,\n",
       " 591: 11,\n",
       " 592: 12,\n",
       " 593: 13,\n",
       " 594: 14,\n",
       " 595: 15,\n",
       " 596: 16,\n",
       " 597: 17,\n",
       " 598: 18,\n",
       " 599: 19,\n",
       " 600: 20,\n",
       " 601: 21,\n",
       " 602: 22,\n",
       " 603: 23,\n",
       " 604: 24,\n",
       " 605: 25,\n",
       " 606: 26,\n",
       " 607: 27,\n",
       " 608: 28,\n",
       " 609: 29,\n",
       " 610: 30,\n",
       " 611: 31,\n",
       " 612: 32,\n",
       " 613: 33,\n",
       " 614: 34,\n",
       " 615: 35,\n",
       " 616: 36,\n",
       " 617: 37,\n",
       " 618: 38,\n",
       " 619: 39,\n",
       " 620: 40,\n",
       " 621: 41,\n",
       " 622: 42,\n",
       " 623: 43,\n",
       " 624: 44,\n",
       " 625: 45,\n",
       " 626: 46,\n",
       " 627: 47,\n",
       " 628: 48,\n",
       " 629: 49,\n",
       " 630: 50,\n",
       " 631: 51,\n",
       " 632: 52,\n",
       " 633: 53,\n",
       " 634: 54,\n",
       " 635: 55,\n",
       " 636: 56,\n",
       " 637: 57,\n",
       " 638: 58,\n",
       " 639: 59,\n",
       " 640: 60,\n",
       " 641: 61,\n",
       " 642: 0,\n",
       " 643: 1,\n",
       " 644: 2,\n",
       " 645: 3,\n",
       " 646: 4,\n",
       " 647: 5,\n",
       " 648: 6,\n",
       " 649: 7,\n",
       " 650: 8,\n",
       " 651: 9,\n",
       " 652: 10,\n",
       " 653: 11,\n",
       " 654: 12,\n",
       " 655: 13,\n",
       " 656: 14,\n",
       " 657: 15,\n",
       " 658: 16,\n",
       " 659: 17,\n",
       " 660: 18,\n",
       " 661: 19,\n",
       " 662: 20,\n",
       " 663: 21,\n",
       " 664: 22,\n",
       " 665: 23,\n",
       " 666: 24,\n",
       " 667: 25,\n",
       " 668: 26,\n",
       " 669: 27,\n",
       " 670: 28,\n",
       " 671: 29,\n",
       " 672: 30,\n",
       " 673: 31,\n",
       " 674: 32,\n",
       " 675: 33,\n",
       " 676: 34,\n",
       " 677: 35,\n",
       " 678: 36,\n",
       " 679: 37,\n",
       " 680: 38,\n",
       " 681: 39,\n",
       " 682: 40,\n",
       " 683: 41,\n",
       " 684: 42,\n",
       " 685: 43,\n",
       " 686: 44,\n",
       " 687: 45,\n",
       " 688: 46,\n",
       " 689: 47,\n",
       " 690: 48,\n",
       " 691: 49,\n",
       " 692: 50,\n",
       " 693: 51,\n",
       " 694: 52,\n",
       " 695: 53,\n",
       " 696: 54,\n",
       " 697: 55,\n",
       " 698: 56,\n",
       " 699: 57,\n",
       " 700: 58,\n",
       " 701: 59,\n",
       " 702: 60,\n",
       " 703: 61,\n",
       " 704: 0,\n",
       " 705: 1,\n",
       " 706: 2,\n",
       " 707: 3,\n",
       " 708: 4,\n",
       " 709: 5,\n",
       " 710: 6,\n",
       " 711: 7,\n",
       " 712: 8,\n",
       " 713: 9,\n",
       " 714: 10,\n",
       " 715: 11,\n",
       " 716: 12,\n",
       " 717: 13,\n",
       " 718: 14,\n",
       " 719: 15,\n",
       " 720: 16,\n",
       " 721: 17,\n",
       " 722: 18,\n",
       " 723: 19,\n",
       " 724: 20,\n",
       " 725: 21,\n",
       " 726: 22,\n",
       " 727: 23,\n",
       " 728: 24,\n",
       " 729: 25,\n",
       " 730: 26,\n",
       " 731: 27,\n",
       " 732: 28,\n",
       " 733: 29,\n",
       " 734: 30,\n",
       " 735: 31,\n",
       " 736: 32,\n",
       " 737: 33,\n",
       " 738: 34,\n",
       " 739: 35,\n",
       " 740: 36,\n",
       " 741: 37,\n",
       " 742: 38,\n",
       " 743: 39,\n",
       " 744: 40,\n",
       " 745: 41,\n",
       " 746: 42,\n",
       " 747: 43,\n",
       " 748: 44,\n",
       " 749: 45,\n",
       " 750: 46,\n",
       " 751: 47,\n",
       " 752: 48,\n",
       " 753: 49,\n",
       " 754: 50,\n",
       " 755: 51,\n",
       " 756: 52,\n",
       " 757: 53,\n",
       " 758: 54,\n",
       " 759: 55,\n",
       " 760: 56,\n",
       " 761: 57,\n",
       " 762: 58,\n",
       " 763: 59,\n",
       " 764: 0,\n",
       " 765: 1,\n",
       " 766: 2,\n",
       " 767: 3,\n",
       " 768: 4,\n",
       " 769: 5,\n",
       " 770: 6,\n",
       " 771: 7,\n",
       " 772: 8,\n",
       " 773: 9,\n",
       " 774: 10,\n",
       " 775: 11,\n",
       " 776: 12,\n",
       " 777: 13,\n",
       " 778: 14,\n",
       " 779: 15,\n",
       " 780: 16,\n",
       " 781: 17,\n",
       " 782: 18,\n",
       " 783: 19,\n",
       " 784: 20,\n",
       " 785: 21,\n",
       " 786: 22,\n",
       " 787: 23,\n",
       " 788: 24,\n",
       " 789: 25,\n",
       " 790: 26,\n",
       " 791: 27,\n",
       " 792: 28,\n",
       " 793: 29,\n",
       " 794: 30,\n",
       " 795: 31,\n",
       " 796: 32,\n",
       " 797: 33,\n",
       " 798: 34,\n",
       " 799: 35,\n",
       " 800: 36,\n",
       " 801: 37,\n",
       " 802: 38,\n",
       " 803: 39,\n",
       " 804: 40,\n",
       " 805: 41,\n",
       " 806: 42,\n",
       " 807: 43,\n",
       " 808: 44,\n",
       " 809: 45,\n",
       " 810: 46,\n",
       " 811: 47,\n",
       " 812: 48,\n",
       " 813: 49,\n",
       " 814: 50,\n",
       " 815: 51,\n",
       " 816: 52,\n",
       " 817: 53,\n",
       " 818: 54,\n",
       " 819: 55,\n",
       " 820: 0,\n",
       " 821: 1,\n",
       " 822: 2,\n",
       " 823: 3,\n",
       " 824: 4,\n",
       " 825: 5,\n",
       " 826: 6,\n",
       " 827: 7,\n",
       " 828: 8,\n",
       " 829: 9,\n",
       " 830: 10,\n",
       " 831: 11,\n",
       " 832: 12,\n",
       " 833: 13,\n",
       " 834: 14,\n",
       " 835: 15,\n",
       " 836: 16,\n",
       " 837: 17,\n",
       " 838: 18,\n",
       " 839: 19,\n",
       " 840: 20,\n",
       " 841: 21,\n",
       " 842: 22,\n",
       " 843: 23,\n",
       " 844: 24,\n",
       " 845: 25,\n",
       " 846: 26,\n",
       " 847: 27,\n",
       " 848: 28,\n",
       " 849: 29,\n",
       " 850: 30,\n",
       " 851: 31,\n",
       " 852: 32,\n",
       " 853: 33,\n",
       " 854: 34,\n",
       " 855: 35,\n",
       " 856: 36,\n",
       " 857: 37,\n",
       " 858: 38,\n",
       " 859: 39,\n",
       " 860: 40,\n",
       " 861: 41,\n",
       " 862: 42,\n",
       " 863: 43,\n",
       " 864: 0,\n",
       " 865: 1,\n",
       " 866: 2,\n",
       " 867: 3,\n",
       " 868: 4,\n",
       " 869: 5,\n",
       " 870: 6,\n",
       " 871: 7,\n",
       " 872: 8,\n",
       " 873: 9,\n",
       " 874: 10,\n",
       " 875: 11,\n",
       " 876: 12,\n",
       " 877: 13,\n",
       " 878: 14,\n",
       " 879: 15,\n",
       " 880: 16,\n",
       " 881: 17,\n",
       " 882: 18,\n",
       " 883: 19,\n",
       " 884: 20,\n",
       " 885: 21,\n",
       " 886: 22,\n",
       " 887: 23,\n",
       " 888: 24,\n",
       " 889: 25,\n",
       " 890: 26,\n",
       " 891: 27,\n",
       " 892: 28,\n",
       " 893: 29,\n",
       " 894: 30,\n",
       " 895: 31,\n",
       " 896: 32,\n",
       " 897: 33,\n",
       " 898: 34,\n",
       " 899: 35,\n",
       " 900: 36,\n",
       " 901: 37,\n",
       " 902: 38,\n",
       " 903: 39,\n",
       " 904: 40,\n",
       " 905: 41,\n",
       " 906: 42,\n",
       " 907: 43,\n",
       " 908: 44,\n",
       " 909: 45,\n",
       " 910: 46,\n",
       " 911: 47,\n",
       " 912: 48,\n",
       " 913: 49,\n",
       " 914: 50,\n",
       " 915: 51,\n",
       " 916: 52,\n",
       " 917: 53,\n",
       " 918: 54,\n",
       " 919: 55,\n",
       " 920: 56,\n",
       " 921: 57,\n",
       " 922: 58,\n",
       " 923: 59,\n",
       " 924: 60,\n",
       " 925: 61,\n",
       " 926: 62,\n",
       " 927: 63,\n",
       " 928: 64,\n",
       " 929: 65,\n",
       " 930: 66,\n",
       " 931: 67,\n",
       " 932: 68,\n",
       " 933: 69,\n",
       " 934: 70,\n",
       " 935: 71,\n",
       " 936: 72,\n",
       " 937: 73,\n",
       " 938: 74,\n",
       " 939: 75,\n",
       " 940: 76,\n",
       " 941: 77,\n",
       " 942: 78,\n",
       " 943: 79,\n",
       " 944: 80,\n",
       " 945: 81,\n",
       " 946: 82,\n",
       " 947: 83,\n",
       " 948: 84,\n",
       " 949: 85,\n",
       " 950: 86,\n",
       " 951: 87,\n",
       " 952: 88,\n",
       " 953: 89,\n",
       " 954: 90,\n",
       " 955: 91,\n",
       " 956: 92,\n",
       " 957: 93,\n",
       " 958: 94,\n",
       " 959: 95,\n",
       " 960: 96,\n",
       " 961: 97,\n",
       " 962: 98,\n",
       " 963: 99,\n",
       " 964: 100,\n",
       " 965: 101,\n",
       " 966: 102,\n",
       " 967: 103,\n",
       " 968: 104,\n",
       " 969: 105,\n",
       " 970: 106,\n",
       " 971: 107,\n",
       " 972: 0,\n",
       " 973: 1,\n",
       " 974: 2,\n",
       " 975: 3,\n",
       " 976: 4,\n",
       " 977: 5,\n",
       " 978: 6,\n",
       " 979: 7,\n",
       " 980: 8,\n",
       " 981: 9,\n",
       " 982: 10,\n",
       " 983: 11,\n",
       " 984: 12,\n",
       " 985: 13,\n",
       " 986: 14,\n",
       " 987: 15,\n",
       " 988: 16,\n",
       " 989: 17,\n",
       " 990: 18,\n",
       " 991: 19,\n",
       " 992: 20,\n",
       " 993: 21,\n",
       " 994: 22,\n",
       " 995: 23,\n",
       " 996: 24,\n",
       " 997: 25,\n",
       " 998: 26,\n",
       " 999: 27,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bae8ba-e853-44e3-a68f-69536f8514bf",
   "metadata": {},
   "source": [
    "Now I have two dicts ready at my disposal to enable the pytorch dataset to process the sequences from the dataset on a certain token instead of generating the whole sequence data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8696186-2f41-4fba-84b1-0bacf494dba0",
   "metadata": {},
   "source": [
    "I'll try this pipeline outside of the class to make sure it can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef09b42a-e181-405f-9b72-3e58e19178eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f40da56-6c16-4a83-bf51-9d0cdf71e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_idx = idx_map[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8be25e3e-b47c-437d-a8c8-2b6c22755fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer.encode(new_dataset[new_idx]['translation']['en'])\n",
    "tokenized_output = tokenizer.encode(new_dataset[new_idx]['translation']['ar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14655004-1a6d-4104-89d9-5688938e2910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24500,    14,   328,  1997,  1932,   613,   407,   545,  3796,     8,\n",
       "           643,   154,  1542,  2632,    14,   175,   176, 10639,   545,  3361,\n",
       "            16,   915,  1542,  6587,    14, 21073,  3477, 13587, 26576,   183,\n",
       "         28628, 27685,  2659, 27087,   372,    15, 18718, 27588, 24816, 24408,\n",
       "          1941, 27048,    14, 25632,  8795,   464,  1997,   143,    51,  4292,\n",
       "            16]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_ids = torch.tensor([tokenized_input.ids[1:-1]])\n",
    "encoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb97d9d2-9b9b-45b1-8596-d9776a05c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    1,  1621, 15566,    86,   378,  1906,   163,  9773,  2196,   151,\n",
       "         18638,  3732,   160, 21807,  8931,  1736,  2329,  1552,    16, 15342,\n",
       "            62,   281,  2380,  1148,  1552,   893,  3719,   189,  3796,     8,\n",
       "           160,  5808,  2630,  6413,  3737,    62,   636,  1426,   629,  9950,\n",
       "           163,   201,  1195,  5762,    16,  1259,  3589,  6629,    17,  3921,\n",
       "           920,  1236,    62,  3223,   245,   163, 14133, 26173, 29258, 28797,\n",
       "            86,  8671,   163,  6603,  2673,   426,    86, 10338, 25380,    62,\n",
       "           738, 13855,   245, 14403,   160, 22567, 23057,   160,  4694,  2834,\n",
       "         13002,    62, 23037, 13369,   210, 22386,  8402,  8808,  9191,    16]),\n",
       " tensor([ 1621, 15566,    86,   378,  1906,   163,  9773,  2196,   151, 18638,\n",
       "          3732,   160, 21807,  8931,  1736,  2329,  1552,    16, 15342,    62,\n",
       "           281,  2380,  1148,  1552,   893,  3719,   189,  3796,     8,   160,\n",
       "          5808,  2630,  6413,  3737,    62,   636,  1426,   629,  9950,   163,\n",
       "           201,  1195,  5762,    16,  1259,  3589,  6629,    17,  3921,   920,\n",
       "          1236,    62,  3223,   245,   163, 14133, 26173, 29258, 28797,    86,\n",
       "          8671,   163,  6603,  2673,   426,    86, 10338, 25380,    62,   738,\n",
       "         13855,   245, 14403,   160, 22567, 23057,   160,  4694,  2834, 13002,\n",
       "            62, 23037, 13369,   210, 22386,  8402,  8808,  9191,    16,     2]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_output_ids = torch.tensor(tokenized_output.ids)\n",
    "decoder_input_ids = tokenized_output_ids[:-1]\n",
    "target_ids = tokenized_output_ids[1:]\n",
    "decoder_input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4316c4c3-33a8-4e0f-b382-038c8c149b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = token_map[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6456e591-d4bb-4a8a-8f92-56f7f11cbdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e0cd3dc-e94b-4510-be99-46ddb13a3d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_masks = torch.ones_like(decoder_input_ids)\n",
    "decoder_input_masks[:token+1] = 0\n",
    "decoder_input_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2132a8dd-2465-4edd-86f5-f1bcb0d7c801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8135, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random = torch.randn(decoder_input_masks.shape)\n",
    "random.masked_fill(decoder_input_masks, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba09b184-3071-461e-b058-b26f90569776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_masks = torch.zeros_like(target_ids)\n",
    "target_masks[:token+1] = 1\n",
    "target_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "532c2600-addc-4fa8-ab01-43a36f64904e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1621,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids * target_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2328ba-0639-4324-8765-05a35b75e98f",
   "metadata": {},
   "source": [
    "So I have tested it with multiple idx and it seems to work pretty well. I'll modify the dataset class to use the new pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50e1d81d-d049-4ebd-936c-555442815bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.init_maps()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_map.keys())\n",
    "\n",
    "    def __getitem__(self, idx):     \n",
    "        new_idx = self.idx_map[idx]\n",
    "        token = self.token_map[idx]\n",
    "        \n",
    "        tokenized_input = tokenizer.encode(self.data[new_idx]['translation']['en'])\n",
    "        tokenized_output = tokenizer.encode(self.data[new_idx]['translation']['ar'])\n",
    "\n",
    "        encoder_input_ids = torch.tensor([tokenized_input.ids[1:-1]])\n",
    "        \n",
    "        tokenized_output_ids = torch.tensor([tokenized_output.ids])\n",
    "\n",
    "        decoder_input_ids = tokenized_output_ids[:, :-1]\n",
    "        decoder_input_masks = self.get_decoder_masks(decoder_input_ids, token)\n",
    "        \n",
    "        \n",
    "        target_ids = tokenized_output_ids[:, 1:]\n",
    "        target_masks = self.get_target_masks(target_ids, token)\n",
    "        \n",
    "        return encoder_input_ids, decoder_input_ids, target_ids, decoder_input_masks, target_masks\n",
    "        \n",
    "\n",
    "    def get_decoder_masks(self, ids, token):\n",
    "        masks = torch.ones_like(ids)\n",
    "        masks[:, :token+1] = 0\n",
    "        return masks\n",
    "    \n",
    "    def get_target_masks(self, ids, token):\n",
    "        masks = torch.zeros_like(ids)\n",
    "        masks[:, :token+1] = 1\n",
    "        return masks\n",
    "    \n",
    "    def count_tokens(self, row):\n",
    "        row['translation']['count'] = len(self.tokenizer.encode(row['translation']['ar']).ids) + 1 # Add one because we use [SOS] and [EOS]\n",
    "        return row\n",
    "    \n",
    "    def init_maps(self):\n",
    "        counts = np.array(self.data.flatten()['translation.count'])\n",
    "        cumsum = np.cumsum(np.pad(counts, (1, 0), 'constant'))\n",
    "        \n",
    "        self.idx_map = {}\n",
    "        for i, (x1, x2) in enumerate(zip(cumsum[:-1], cumsum[1:])):\n",
    "            for n in np.arange(x1, x2):\n",
    "                self.idx_map[n] = i\n",
    "                \n",
    "        self.token_map = {}\n",
    "        for x1, x2 in zip(cumsum[:-1], cumsum[1:]):\n",
    "            for i, n in enumerate(np.arange(x1, x2)):\n",
    "                self.token_map[n] = i\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4bb68af1-0a21-4703-8b58-c8b437197b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6891959"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TransformerDataset(new_dataset, tokenizer)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a65beda-56bc-470b-b924-ed03959f250c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   7, 2261,   14, 2427, 1997,   30]]),\n",
       " tensor([[    1,  1552, 20144,  1005,  1316,    64]]),\n",
       " tensor([[ 1552, 20144,  1005,  1316,    64,     2]]),\n",
       " tensor([[0, 1, 1, 1, 1, 1]]),\n",
       " tensor([[1, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec7f967d-5c10-45ef-9c8b-639d62cc2d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   7, 2261,   14, 2427, 1997,   30]]),\n",
       " tensor([[    1,  1552, 20144,  1005,  1316,    64]]),\n",
       " tensor([[ 1552, 20144,  1005,  1316,    64,     2]]),\n",
       " tensor([[0, 0, 1, 1, 1, 1]]),\n",
       " tensor([[1, 1, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e314b8db-dbe8-4f93-af0f-a10f771b4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_row(r, max_seq_len):\n",
    "    \"Pads each row from Dataset __getitem__ to maximum sequence length in the current batch\"\n",
    "    return [F.pad(r[i], (0, max_seq_len[i] - r[i].shape[1]), \"constant\", 0) for i in range(len(r))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b0958a43-f13d-4add-af77-934778e8347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"Collates multiples tuples into one tensor by padding their sequence length and concatenating them\"\n",
    "    max_seq_len = [max([t.shape[1] for t in ts]) for ts in zip(*batch)]\n",
    "    padded_batch = [pad_row(r, max_seq_len) for r in batch]\n",
    "    return [torch.concat(ts, dim=0) for ts in zip(*padded_batch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1150ea03-da04-4119-8acc-dcd5b553c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [dataset[i] for i in range(6, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15dfaa5a-9a68-4430-a61a-f5c164074e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[torch.Size([1, 6]),\n",
       "  torch.Size([1, 6]),\n",
       "  torch.Size([1, 6]),\n",
       "  torch.Size([1, 6]),\n",
       "  torch.Size([1, 6])],\n",
       " [torch.Size([1, 6]),\n",
       "  torch.Size([1, 6]),\n",
       "  torch.Size([1, 6]),\n",
       "  torch.Size([1, 6]),\n",
       "  torch.Size([1, 6])],\n",
       " [torch.Size([1, 51]),\n",
       "  torch.Size([1, 90]),\n",
       "  torch.Size([1, 90]),\n",
       "  torch.Size([1, 90]),\n",
       "  torch.Size([1, 90])]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[t.shape for t in r] for r in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5cabddfe-c319-4b06-8d0c-1bd71c3266b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([3, 51]),\n",
       " torch.Size([3, 90]),\n",
       " torch.Size([3, 90]),\n",
       " torch.Size([3, 90]),\n",
       " torch.Size([3, 90])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.shape for t in collate_fn(batch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fa4a351b-ec66-49fa-a2fc-7fd2ed7f0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import wandb\n",
    "\n",
    "def train_one_epoch(epoch_index, model, optimizer, loss_fn, training_loader, validation_loader, save_path):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    for i, data in enumerate(training_loader):\n",
    "        encoder_input_ids, decoder_input_ids, target_ids, decoder_input_masks, target_masks = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(encoder_input_ids.to('cuda'), decoder_input_ids.to('cuda'), decoder_input_masks.to('cuda').bool())\n",
    "        \n",
    "        outputs = outputs.view(-1, tokenizer.get_vocab_size())\n",
    "\n",
    "        loss = loss_fn(outputs[target_masks.flatten().to('cuda').bool()],\n",
    "                       target_ids.flatten().to('cuda')[target_masks.flatten().to('cuda').bool()])\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        n = 1000\n",
    "        \n",
    "        if i % n == n - 1:    # print every n mini-batches\n",
    "            print('  batch {} loss: {}'.format(i + 1, running_loss/n))\n",
    "            \n",
    "            wandb.log({\"running_loss\": running_loss/n})\n",
    "            \n",
    "            if running_loss < last_loss:\n",
    "                torch.save({\n",
    "                    'epoch': epoch_index,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': running_loss,\n",
    "                }, save_path)\n",
    "                \n",
    "            last_loss = running_loss\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        \n",
    "        del encoder_input_ids\n",
    "        del decoder_input_ids\n",
    "        del target_ids\n",
    "        del decoder_input_masks\n",
    "        del target_masks\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    wandb.log({\"train_loss\": epoch_loss/len(training_loader)})\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_loss = 0.\n",
    "    \n",
    "        for i, data in enumerate(validation_loader):\n",
    "            encoder_input_ids, decoder_input_ids, target_ids, decoder_input_masks, target_masks = data\n",
    "\n",
    "            outputs = model(encoder_input_ids.to('cuda'), decoder_input_ids.to('cuda'), decoder_input_masks.to('cuda').bool())\n",
    "\n",
    "            outputs = outputs.view(-1, tokenizer.get_vocab_size())\n",
    "\n",
    "            loss = loss_fn(outputs[target_masks.flatten().to('cuda').bool()],\n",
    "                           target_ids.flatten().to('cuda')[target_masks.flatten().to('cuda').bool()])\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            del encoder_input_ids\n",
    "            del decoder_input_ids\n",
    "            del target_ids\n",
    "            del decoder_input_masks\n",
    "            del target_masks\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        wandb.log({\"val_loss\": val_loss/len(validation_loader)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67574802-08f5-4b51-827d-bb3df2bb7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8cfc99ff-a2fb-442c-832d-594721035279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2j1qagbz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇█▇▅▅▄▄▄▃▃▂▃▂▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>8.14026</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">frosty-firebrand-3</strong>: <a href=\"https://wandb.ai/ahmedsamirio/attention-is-all-you-need/runs/2j1qagbz\" target=\"_blank\">https://wandb.ai/ahmedsamirio/attention-is-all-you-need/runs/2j1qagbz</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220815_175013-2j1qagbz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2j1qagbz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20220815_175133-22o4byul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ahmedsamirio/attention-is-all-you-need/runs/22o4byul\" target=\"_blank\">gallant-dream-4</a></strong> to <a href=\"https://wandb.ai/ahmedsamirio/attention-is-all-you-need\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/ahmedsamirio/attention-is-all-you-need/runs/22o4byul?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f792760a130>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"attention-is-all-you-need\", entity=\"ahmedsamirio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20105a44-90da-4b39-9cc2-6fc9b58e8dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 83187\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cbaf8210-9fd2-4623-896a-f35c304b0b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.select([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aa4f311d-0683-4eca-b9ef-b7a5c628b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(len(new_dataset)*0.8)\n",
    "train_ids = range(0, train_len)\n",
    "valid_ids = range(train_len, len(new_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1676de16-73e7-4426-b591-4e6e295780b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset = TransformerDataset(new_dataset.select(train_ids), tokenizer)\n",
    "ValidDataset = TransformerDataset(new_dataset.select(valid_ids), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "153bcf53-b4d7-458e-9b4d-9cabc067b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(tokenizer.get_vocab_size(), debug=False).to('cuda')\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "training_loader = DataLoader(TrainDataset, batch_size=64, collate_fn=collate_fn, shuffle=False)\n",
    "validation_loader = DataLoader(ValidDataset, batch_size=64, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38174eda-12b5-45e8-a38f-d0ad45a7cf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 12.10929033613205\n",
      "  batch 2000 loss: 8.59061136817932\n",
      "  batch 3000 loss: 8.168880687713623\n",
      "  batch 4000 loss: 7.952878517627716\n",
      "  batch 5000 loss: 7.993534821033478\n",
      "  batch 6000 loss: 8.048997934818267\n",
      "  batch 7000 loss: 7.8977878718376155\n",
      "  batch 8000 loss: 7.84813807964325\n",
      "  batch 9000 loss: 7.89777515411377\n",
      "  batch 10000 loss: 7.861690134525299\n",
      "  batch 11000 loss: 7.831814856529236\n",
      "  batch 12000 loss: 7.81308338546753\n",
      "  batch 13000 loss: 7.821840600967407\n",
      "  batch 14000 loss: 7.821160840988159\n",
      "  batch 15000 loss: 7.759237428188324\n",
      "  batch 16000 loss: 7.706367583751678\n",
      "  batch 17000 loss: 7.715282855033874\n",
      "  batch 18000 loss: 7.748741548538208\n",
      "  batch 19000 loss: 7.849751022338867\n",
      "  batch 20000 loss: 7.809839839935303\n",
      "  batch 21000 loss: 7.815505774021148\n",
      "  batch 22000 loss: 7.832446142673493\n",
      "  batch 23000 loss: 7.793722000598907\n",
      "  batch 24000 loss: 7.763119186401367\n",
      "  batch 25000 loss: 7.776265001773834\n",
      "  batch 26000 loss: 7.802763346195221\n",
      "  batch 27000 loss: 7.791239711284637\n",
      "  batch 28000 loss: 7.754101455688477\n",
      "  batch 29000 loss: 7.723297582626342\n",
      "  batch 30000 loss: 7.669399088859558\n",
      "  batch 31000 loss: 7.7866721205711364\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    train_one_epoch(i, transformer, optimizer, loss_fn, training_loader, validation_loader, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5848b2-3064-4e9e-adda-3b639e057828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
